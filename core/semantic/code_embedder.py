#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
JARVIS v14 Ultimate - Semantic Code Embedder
=============================================

Phase 2: Generate semantic embeddings for code understanding.

This module converts code into semantic representations that enable:
- Similarity detection between code snippets
- Code clustering by functionality
- Intent understanding
- Cross-file analysis

Key Features:
- AST-based structural features
- Semantic description via Kimi K2.5
- Efficient caching for performance
- Termux-friendly (no heavy ML models)

Author: JARVIS AI Project
Version: 2.0.0
Target Level: 40-50
Device: Realme Pad 2 Lite | Termux
"""

import ast
import json
import hashlib
import logging
import threading
import time
import re
from typing import Dict, Any, Optional, List, Tuple, Set
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from collections import Counter

logger = logging.getLogger(__name__)


# ═══════════════════════════════════════════════════════════════════════════════
# DATACLASSES
# ═══════════════════════════════════════════════════════════════════════════════

@dataclass
class StructuralFeatures:
    """
    AST-extracted structural features of code.
    
    These features capture the structure and patterns in code
    without needing heavy ML models.
    """
    # Function/class counts
    function_count: int = 0
    class_count: int = 0
    async_function_count: int = 0
    
    # Complexity metrics
    max_nesting_depth: int = 0
    total_lines: int = 0
    code_lines: int = 0
    comment_lines: int = 0
    
    # Patterns detected
    has_try_except: bool = False
    has_context_manager: bool = False
    has_decorator: bool = False
    has_comprehension: bool = False
    has_generator: bool = False
    has_lambda: bool = False
    
    # Imports
    import_count: int = 0
    imports: List[str] = field(default_factory=list)
    
    # Function signatures
    function_names: List[str] = field(default_factory=list)
    class_names: List[str] = field(default_factory=list)
    
    # Variable patterns
    variable_names: List[str] = field(default_factory=list)
    
    # Keywords used
    keywords_used: Set[str] = field(default_factory=set)
    
    # API calls
    api_calls: List[str] = field(default_factory=list)
    
    # Docstrings
    has_docstrings: bool = False
    docstring_count: int = 0
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization"""
        return {
            'function_count': self.function_count,
            'class_count': self.class_count,
            'async_function_count': self.async_function_count,
            'max_nesting_depth': self.max_nesting_depth,
            'total_lines': self.total_lines,
            'code_lines': self.code_lines,
            'comment_lines': self.comment_lines,
            'has_try_except': self.has_try_except,
            'has_context_manager': self.has_context_manager,
            'has_decorator': self.has_decorator,
            'has_comprehension': self.has_comprehension,
            'has_generator': self.has_generator,
            'has_lambda': self.has_lambda,
            'import_count': self.import_count,
            'imports': self.imports,
            'function_names': self.function_names,
            'class_names': self.class_names,
            'has_docstrings': self.has_docstrings,
            'docstring_count': self.docstring_count,
            'keywords_used': list(self.keywords_used),
        }
    
    def to_feature_vector(self) -> List[float]:
        """Convert to numerical feature vector for similarity"""
        return [
            float(self.function_count),
            float(self.class_count),
            float(self.async_function_count),
            float(self.max_nesting_depth),
            float(self.total_lines) / 100.0,  # Normalize
            float(self.code_lines) / 100.0,
            float(self.import_count),
            float(self.has_try_except),
            float(self.has_context_manager),
            float(self.has_decorator),
            float(self.has_comprehension),
            float(self.has_generator),
            float(self.has_lambda),
            float(self.has_docstrings),
            float(self.docstring_count),
        ]


@dataclass
class SemanticDescription:
    """
    AI-generated semantic description of code.
    
    Generated by Kimi K2.5 for deep understanding.
    """
    summary: str = ""
    purpose: str = ""
    patterns: List[str] = field(default_factory=list)
    dependencies: List[str] = field(default_factory=list)
    complexity_level: str = "medium"  # low, medium, high, very_high
    quality_score: float = 0.5
    issues: List[str] = field(default_factory=list)
    suggestions: List[str] = field(default_factory=list)
    keywords: List[str] = field(default_factory=list)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'summary': self.summary,
            'purpose': self.purpose,
            'patterns': self.patterns,
            'dependencies': self.dependencies,
            'complexity_level': self.complexity_level,
            'quality_score': self.quality_score,
            'issues': self.issues,
            'suggestions': self.suggestions,
            'keywords': self.keywords,
        }


@dataclass
class CodeEmbedding:
    """
    Complete code embedding with all metadata.
    
    This is the main artifact produced by the embedder.
    """
    # Identification
    code_hash: str
    file_path: Optional[str] = None
    function_name: Optional[str] = None
    
    # Features
    structural_features: StructuralFeatures = None
    semantic_description: SemanticDescription = None
    
    # Embedding vector (for similarity search)
    embedding_vector: List[float] = field(default_factory=list)
    
    # Metadata
    language: str = "python"
    created_at: float = field(default_factory=time.time)
    last_accessed: float = field(default_factory=time.time)
    access_count: int = 0
    
    # Size info
    char_count: int = 0
    line_count: int = 0
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization"""
        return {
            'code_hash': self.code_hash,
            'file_path': self.file_path,
            'function_name': self.function_name,
            'structural_features': self.structural_features.to_dict() if self.structural_features else None,
            'semantic_description': self.semantic_description.to_dict() if self.semantic_description else None,
            'embedding_vector': self.embedding_vector[:20] if self.embedding_vector else [],  # Truncated
            'language': self.language,
            'created_at': self.created_at,
            'char_count': self.char_count,
            'line_count': self.line_count,
        }
    
    def get_feature_hash(self) -> str:
        """Get hash of structural features for quick comparison"""
        if not self.structural_features:
            return ""
        
        feature_str = json.dumps(self.structural_features.to_dict(), sort_keys=True)
        return hashlib.md5(feature_str.encode()).hexdigest()[:16]


# ═══════════════════════════════════════════════════════════════════════════════
# EMBEDDING CACHE
# ═══════════════════════════════════════════════════════════════════════════════

class EmbeddingCache:
    """
    Thread-safe cache for code embeddings.
    
    Features:
    - LRU eviction
    - TTL-based expiration
    - Memory limits for Termux
    - Persistent storage support
    """
    
    MAX_CACHE_SIZE = 1000  # Maximum embeddings in memory
    MAX_MEMORY_MB = 50     # Maximum memory usage in MB
    
    def __init__(self, max_size: int = None, ttl_seconds: int = 86400):
        """
        Initialize embedding cache.
        
        Args:
            max_size: Maximum number of embeddings to cache
            ttl_seconds: Time-to-live for cache entries
        """
        self._cache: Dict[str, Tuple[CodeEmbedding, float]] = {}
        self._max_size = max_size or self.MAX_CACHE_SIZE
        self._ttl = ttl_seconds
        self._lock = threading.RLock()
        
        # Statistics
        self._stats = {
            'hits': 0,
            'misses': 0,
            'evictions': 0,
            'total_stored': 0,
        }
    
    def get(self, code_hash: str) -> Optional[CodeEmbedding]:
        """Get embedding from cache"""
        with self._lock:
            if code_hash in self._cache:
                embedding, timestamp = self._cache[code_hash]
                
                # Check TTL
                if time.time() - timestamp > self._ttl:
                    del self._cache[code_hash]
                    self._stats['misses'] += 1
                    return None
                
                # Update access time
                embedding.last_accessed = time.time()
                embedding.access_count += 1
                self._stats['hits'] += 1
                return embedding
            
            self._stats['misses'] += 1
            return None
    
    def put(self, embedding: CodeEmbedding) -> None:
        """Store embedding in cache"""
        with self._lock:
            # Evict if needed
            while len(self._cache) >= self._max_size:
                self._evict_oldest()
            
            self._cache[embedding.code_hash] = (embedding, time.time())
            self._stats['total_stored'] += 1
    
    def _evict_oldest(self):
        """Evict oldest/least recently used entry"""
        if not self._cache:
            return
        
        # Find oldest by last_accessed
        oldest_hash = min(
            self._cache.keys(),
            key=lambda h: self._cache[h][0].last_accessed
        )
        del self._cache[oldest_hash]
        self._stats['evictions'] += 1
    
    def invalidate(self, code_hash: str) -> bool:
        """Remove embedding from cache"""
        with self._lock:
            if code_hash in self._cache:
                del self._cache[code_hash]
                return True
            return False
    
    def clear(self):
        """Clear all cached embeddings"""
        with self._lock:
            self._cache.clear()
    
    def get_stats(self) -> Dict[str, Any]:
        """Get cache statistics"""
        with self._lock:
            stats = self._stats.copy()
            stats['size'] = len(self._cache)
            stats['hit_rate'] = (
                stats['hits'] / (stats['hits'] + stats['misses'])
                if (stats['hits'] + stats['misses']) > 0 else 0
            )
            return stats


# ═══════════════════════════════════════════════════════════════════════════════
# STRUCTURAL FEATURE EXTRACTOR
# ═══════════════════════════════════════════════════════════════════════════════

class StructuralFeatureExtractor:
    """
    Extract structural features from code using AST.
    
    This provides fast, deterministic feature extraction
    without needing AI models.
    """
    
    # Python keywords to track
    PYTHON_KEYWORDS = {
        'if', 'elif', 'else', 'for', 'while', 'try', 'except',
        'finally', 'with', 'as', 'def', 'class', 'return',
        'yield', 'raise', 'assert', 'break', 'continue',
        'pass', 'import', 'from', 'global', 'nonlocal',
        'lambda', 'await', 'async'
    }
    
    def extract(self, code: str, language: str = "python") -> StructuralFeatures:
        """
        Extract structural features from code.
        
        Args:
            code: Source code to analyze
            language: Programming language
            
        Returns:
            StructuralFeatures object
        """
        features = StructuralFeatures()
        
        if language != "python":
            return self._extract_generic(code, features)
        
        # Line counts
        lines = code.split('\n')
        features.total_lines = len(lines)
        features.code_lines = sum(1 for line in lines if line.strip() and not line.strip().startswith('#'))
        features.comment_lines = sum(1 for line in lines if line.strip().startswith('#'))
        
        # Parse AST
        try:
            tree = ast.parse(code)
            self._extract_from_ast(tree, features)
        except SyntaxError as e:
            logger.warning(f"Syntax error in code: {e}")
            features = self._extract_generic(code, features)
        
        return features
    
    def _extract_from_ast(self, tree: ast.AST, features: StructuralFeatures) -> None:
        """Extract features from Python AST"""
        
        for node in ast.walk(tree):
            # Count functions
            if isinstance(node, ast.FunctionDef):
                features.function_count += 1
                features.function_names.append(node.name)
                features.keywords_used.add('def')
                
                # Check for docstring
                if (node.body and isinstance(node.body[0], ast.Expr) and
                    isinstance(node.body[0].value, ast.Constant) and
                    isinstance(node.body[0].value.value, str)):
                    features.has_docstrings = True
                    features.docstring_count += 1
            
            # Count async functions
            elif isinstance(node, ast.AsyncFunctionDef):
                features.async_function_count += 1
                features.function_names.append(node.name)
                features.keywords_used.add('async')
                features.keywords_used.add('def')
            
            # Count classes
            elif isinstance(node, ast.ClassDef):
                features.class_count += 1
                features.class_names.append(node.name)
                features.keywords_used.add('class')
                
                # Check for docstring
                if (node.body and isinstance(node.body[0], ast.Expr) and
                    isinstance(node.body[0].value, ast.Constant) and
                    isinstance(node.body[0].value.value, str)):
                    features.has_docstrings = True
                    features.docstring_count += 1
            
            # Check for decorators
            elif isinstance(node, (ast.FunctionDef, ast.ClassDef)):
                if node.decorator_list:
                    features.has_decorator = True
            
            # Check for try/except
            elif isinstance(node, ast.Try):
                features.has_try_except = True
                features.keywords_used.add('try')
                features.keywords_used.add('except')
            
            # Check for context managers
            elif isinstance(node, ast.With):
                features.has_context_manager = True
                features.keywords_used.add('with')
            
            elif isinstance(node, ast.AsyncWith):
                features.has_context_manager = True
                features.keywords_used.update(['async', 'with'])
            
            # Check for comprehensions
            elif isinstance(node, (ast.ListComp, ast.SetComp, ast.DictComp, ast.GeneratorExp)):
                features.has_comprehension = True
            
            # Check for generators
            elif isinstance(node, ast.Yield):
                features.has_generator = True
                features.keywords_used.add('yield')
            
            elif isinstance(node, ast.YieldFrom):
                features.has_generator = True
                features.keywords_used.update(['yield', 'from'])
            
            # Check for lambda
            elif isinstance(node, ast.Lambda):
                features.has_lambda = True
                features.keywords_used.add('lambda')
            
            # Collect imports
            elif isinstance(node, ast.Import):
                features.import_count += 1
                for alias in node.names:
                    features.imports.append(alias.name)
                features.keywords_used.add('import')
            
            elif isinstance(node, ast.ImportFrom):
                features.import_count += 1
                module = node.module or ''
                for alias in node.names:
                    features.imports.append(f"{module}.{alias.name}" if module else alias.name)
                features.keywords_used.update(['from', 'import'])
            
            # Collect variable names
            elif isinstance(node, ast.Name) and isinstance(node.ctx, ast.Store):
                features.variable_names.append(node.id)
            
            # Collect API calls
            elif isinstance(node, ast.Call):
                call_name = self._get_call_name(node)
                if call_name:
                    features.api_calls.append(call_name)
        
        # Calculate max nesting depth
        features.max_nesting_depth = self._calculate_nesting_depth(tree)
    
    def _get_call_name(self, node: ast.Call) -> str:
        """Get the name of a function call"""
        if isinstance(node.func, ast.Name):
            return node.func.id
        elif isinstance(node.func, ast.Attribute):
            parts = []
            current = node.func
            while isinstance(current, ast.Attribute):
                parts.append(current.attr)
                current = current.value
            if isinstance(current, ast.Name):
                parts.append(current.id)
            return '.'.join(reversed(parts))
        return ""
    
    def _calculate_nesting_depth(self, tree: ast.AST) -> int:
        """Calculate maximum nesting depth"""
        def visit(node: ast.AST, depth: int) -> int:
            max_depth = depth
            for child in ast.iter_child_nodes(node):
                if isinstance(child, (ast.If, ast.For, ast.While, ast.With, ast.Try)):
                    child_depth = visit(child, depth + 1)
                else:
                    child_depth = visit(child, depth)
                max_depth = max(max_depth, child_depth)
            return max_depth
        
        return visit(tree, 0)
    
    def _extract_generic(self, code: str, features: StructuralFeatures) -> StructuralFeatures:
        """Extract basic features when AST parsing fails"""
        lines = code.split('\n')
        features.total_lines = len(lines)
        features.code_lines = sum(1 for line in lines if line.strip())
        
        # Simple pattern matching
        features.function_count = len(re.findall(r'\bdef\s+\w+', code))
        features.class_count = len(re.findall(r'\bclass\s+\w+', code))
        features.has_try_except = 'try:' in code or 'except' in code
        features.has_context_manager = 'with ' in code
        features.import_count = len(re.findall(r'\bimport\s+\w+', code))
        
        return features


# ═══════════════════════════════════════════════════════════════════════════════
# SEMANTIC CODE EMBEDDER
# ═══════════════════════════════════════════════════════════════════════════════

class SemanticCodeEmbedder:
    """
    Generate semantic embeddings for code understanding.
    
    This is the main class for Phase 2 - Semantic Code Understanding.
    It combines:
    - Structural feature extraction (AST-based, fast)
    - Semantic description (AI-based, deep understanding)
    - Embedding generation (for similarity search)
    
    Usage:
        embedder = SemanticCodeEmbedder(kimi_client)
        embedding = embedder.embed(code)
        
        # Find similar code
        similar = embedder.find_similar(embedding, corpus)
    """
    
    def __init__(
        self,
        kimi_client=None,
        cache_size: int = 1000,
        enable_ai_description: bool = True,
        timeout: int = 120,
    ):
        """
        Initialize semantic code embedder.
        
        Args:
            kimi_client: Kimi K2.5 client for AI descriptions
            cache_size: Maximum cache size
            enable_ai_description: Whether to generate AI descriptions
            timeout: Timeout for AI calls
        """
        self._kimi = kimi_client
        self._enable_ai = enable_ai_description
        self._timeout = timeout
        
        # Components
        self._feature_extractor = StructuralFeatureExtractor()
        self._cache = EmbeddingCache(max_size=cache_size)
        
        # Statistics
        self._stats = {
            'total_embeddings': 0,
            'cache_hits': 0,
            'ai_descriptions': 0,
            'failed_descriptions': 0,
            'avg_processing_time_ms': 0.0,
        }
        
        logger.info("SemanticCodeEmbedder initialized")
    
    def set_kimi_client(self, client):
        """Set Kimi client (lazy initialization)"""
        self._kimi = client
    
    def embed(
        self,
        code: str,
        file_path: str = None,
        function_name: str = None,
        skip_ai: bool = False,
    ) -> CodeEmbedding:
        """
        Generate semantic embedding for code.
        
        Args:
            code: Source code to embed
            file_path: Optional file path for context
            function_name: Optional function name being embedded
            skip_ai: Skip AI description generation
            
        Returns:
            CodeEmbedding with all features
        """
        start_time = time.time()
        
        # Generate hash
        code_hash = hashlib.sha256(code.encode()).hexdigest()
        
        # Check cache
        cached = self._cache.get(code_hash)
        if cached:
            self._stats['cache_hits'] += 1
            return cached
        
        # Create embedding
        embedding = CodeEmbedding(
            code_hash=code_hash,
            file_path=file_path,
            function_name=function_name,
            char_count=len(code),
            line_count=len(code.split('\n')),
        )
        
        # Extract structural features
        embedding.structural_features = self._feature_extractor.extract(code)
        
        # Generate AI description if enabled
        if self._enable_ai and self._kimi and not skip_ai:
            embedding.semantic_description = self._generate_ai_description(code)
            self._stats['ai_descriptions'] += 1
        
        # Generate embedding vector
        embedding.embedding_vector = self._generate_embedding_vector(embedding)
        
        # Cache result
        self._cache.put(embedding)
        
        # Update stats
        processing_time = (time.time() - start_time) * 1000
        self._stats['total_embeddings'] += 1
        self._stats['avg_processing_time_ms'] = (
            (self._stats['avg_processing_time_ms'] * (self._stats['total_embeddings'] - 1) + processing_time)
            / self._stats['total_embeddings']
        )
        
        return embedding
    
    def embed_file(self, file_path: str) -> List[CodeEmbedding]:
        """
        Generate embeddings for all functions in a file.
        
        Args:
            file_path: Path to Python file
            
        Returns:
            List of CodeEmbedding for each function
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                code = f.read()
        except Exception as e:
            logger.error(f"Error reading file {file_path}: {e}")
            return []
        
        embeddings = []
        
        try:
            tree = ast.parse(code)
            
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    # Get function source
                    func_source = ast.get_source_segment(code, node)
                    if func_source:
                        embedding = self.embed(
                            func_source,
                            file_path=file_path,
                            function_name=node.name,
                        )
                        embeddings.append(embedding)
                        
        except SyntaxError as e:
            logger.warning(f"Syntax error in {file_path}: {e}")
        
        return embeddings
    
    def _generate_ai_description(self, code: str) -> SemanticDescription:
        """Generate AI-powered semantic description"""
        if not self._kimi:
            return SemanticDescription()
        
        prompt = f"""Analyze this code and provide a structured description.

Code:
```python
{code[:4000]}  # Truncate for token limits
```

Provide your analysis in this JSON format:
{{
    "summary": "One-line summary of what the code does",
    "purpose": "Detailed explanation of the code's purpose",
    "patterns": ["list of", "design patterns", "used"],
    "dependencies": ["list of", "external dependencies"],
    "complexity_level": "low/medium/high/very_high",
    "quality_score": 0.0-1.0,
    "issues": ["list of potential", "issues or bugs"],
    "suggestions": ["list of improvement", "suggestions"],
    "keywords": ["list of relevant", "keywords for search"]
}}

Output ONLY the JSON, no other text."""
        
        try:
            response = self._kimi.chat(
                prompt,
                temperature=0.3,
                max_tokens=1024,
            )
            
            if response.success:
                # Extract JSON from response
                json_match = re.search(r'\{.*\}', response.content, re.DOTALL)
                if json_match:
                    data = json.loads(json_match.group())
                    return SemanticDescription(
                        summary=data.get('summary', ''),
                        purpose=data.get('purpose', ''),
                        patterns=data.get('patterns', []),
                        dependencies=data.get('dependencies', []),
                        complexity_level=data.get('complexity_level', 'medium'),
                        quality_score=float(data.get('quality_score', 0.5)),
                        issues=data.get('issues', []),
                        suggestions=data.get('suggestions', []),
                        keywords=data.get('keywords', []),
                    )
        except Exception as e:
            logger.warning(f"AI description failed: {e}")
            self._stats['failed_descriptions'] += 1
        
        return SemanticDescription()
    
    def _generate_embedding_vector(self, embedding: CodeEmbedding) -> List[float]:
        """Generate embedding vector from features"""
        vector = []
        
        # Add structural features
        if embedding.structural_features:
            vector.extend(embedding.structural_features.to_feature_vector())
        
        # Add semantic features if available
        if embedding.semantic_description:
            # Encode complexity level
            complexity_map = {'low': 0.25, 'medium': 0.5, 'high': 0.75, 'very_high': 1.0}
            vector.append(complexity_map.get(embedding.semantic_description.complexity_level, 0.5))
            vector.append(embedding.semantic_description.quality_score)
            vector.append(float(len(embedding.semantic_description.patterns)))
            vector.append(float(len(embedding.semantic_description.keywords)))
        else:
            # Default values
            vector.extend([0.5, 0.5, 0.0, 0.0])
        
        # Normalize vector
        if vector:
            magnitude = sum(x**2 for x in vector) ** 0.5
            if magnitude > 0:
                vector = [x / magnitude for x in vector]
        
        return vector
    
    def get_stats(self) -> Dict[str, Any]:
        """Get embedder statistics"""
        stats = self._stats.copy()
        stats['cache_stats'] = self._cache.get_stats()
        return stats
    
    def clear_cache(self):
        """Clear embedding cache"""
        self._cache.clear()


# ═══════════════════════════════════════════════════════════════════════════════
# GLOBAL INSTANCE
# ═══════════════════════════════════════════════════════════════════════════════

_embedder: Optional[SemanticCodeEmbedder] = None


def get_embedder(kimi_client=None) -> SemanticCodeEmbedder:
    """Get or create global embedder"""
    global _embedder
    if _embedder is None:
        _embedder = SemanticCodeEmbedder(kimi_client=kimi_client)
    elif kimi_client:
        _embedder.set_kimi_client(kimi_client)
    return _embedder


# ═══════════════════════════════════════════════════════════════════════════════
# SELF TEST
# ═══════════════════════════════════════════════════════════════════════════════

def self_test():
    """Run self-test"""
    print("\n" + "="*60)
    print("Semantic Code Embedder Test")
    print("="*60)
    
    # Test code
    test_code = '''
def calculate_fibonacci(n: int) -> list:
    """Calculate Fibonacci sequence up to n terms."""
    if n <= 0:
        return []
    elif n == 1:
        return [0]
    
    sequence = [0, 1]
    while len(sequence) < n:
        sequence.append(sequence[-1] + sequence[-2])
    
    return sequence
'''
    
    # Create embedder without AI for fast test
    embedder = SemanticCodeEmbedder(kimi_client=None, enable_ai_description=False)
    
    print("\nTest 1: Embed code...")
    embedding = embedder.embed(test_code, function_name="calculate_fibonacci")
    
    print(f"✓ Code hash: {embedding.code_hash[:16]}...")
    print(f"✓ Features: {embedding.structural_features.to_dict()}")
    print(f"✓ Vector size: {len(embedding.embedding_vector)}")
    print(f"✓ Line count: {embedding.line_count}")
    
    print("\nTest 2: Cache test...")
    cached = embedder.embed(test_code)  # Should hit cache
    stats = embedder.get_stats()
    print(f"✓ Cache hits: {stats['cache_hits']}")
    
    print("\n" + "="*60)


if __name__ == "__main__":
    self_test()
